{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datafile_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data = pd.read_csv('datafile_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data = extra_data[['studyid', 'mrn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(df, trunc_data, on='studyid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 1A  (predict rvedvi_hi from ASE guideline dataset)#####\n",
    "\n",
    "inputs_1a = ['rvenddiastolicareacm2ap',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvmidenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvendsystolicareacm2ap4',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'tvannulusdimensionindiastole',\n",
    "'bsaonmri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alter data\n",
    "\n",
    "# alter pulmonaryoutflowtypeattimeo and degreeoftrbyechoreportif, get rid of sex\n",
    "df_2 = merge_df\n",
    "#pulm outflow - want is to be native (1), trans annular patch (2), and other (rest of #s)\n",
    "new_pulm_col = []\n",
    "for i in df_2['pulmonaryoutflowtypeattimeo']:\n",
    "    if i == 1 or i == 2:\n",
    "        new_pulm_col.append(i)\n",
    "    else:\n",
    "        new_pulm_col.append(3)\n",
    "        \n",
    "df_2['new_pulm_outflow_col'] = new_pulm_col\n",
    "\n",
    "#tr - greater than mild (3+) vs not (0,1,2)\n",
    "new_tr_col = []\n",
    "for i in df_2['degreeoftrbyechoreportif']:\n",
    "    if i == 1 or i == 2 or i == 0:\n",
    "        new_tr_col.append(0)\n",
    "    else:\n",
    "        new_tr_col.append(1)\n",
    "        \n",
    "df_2['tr_greater_than_mild'] = new_tr_col\n",
    "\n",
    "df_2 = df_2.drop(columns=['sex', 'pulmonaryoutflowtypeattimeo', 'degreeoftrbyechoreportif'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['tr_greater_than_mild'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all 1A stuff\n",
    "average_performance_1A = average_performance\n",
    "std_dev_performance_1A = std_dev_performance\n",
    "mean_tpr_1A = mean_tpr\n",
    "std_tpr_1A = std_tpr\n",
    "mean_fprs_1A = mean_fprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc with group stratification and cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data setup\n",
    "X = merge_df[inputs_1a]\n",
    "y = merge_df['outcome_rvedvi_hi']\n",
    "groups = merge_df['mrn']  # Group identifier\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Number of splits for cross-validation\n",
    "n_splits_outer = 5\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_cv = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists for storing results\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)\n",
    "tpr_interpolations = []\n",
    "\n",
    "#store feature importance per fold\n",
    "feature_importances = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,  # You can adjust this value\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    #get feature importance for fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(fi)\n",
    "\n",
    "# Average auroc across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', lw=2, label=f'Mean ROC (AUC = {average_performance:.2f})')\n",
    "plt.fill_between(mean_fprs, mean_tpr - std_tpr, mean_tpr + std_tpr, color='grey', alpha=0.2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#xgboost auroc with nested cross validation \n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df[inputs_1a]\n",
    "y = df['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "#make list to store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    #save feature importances for this fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(temp_fi)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(mean_fprs_1A, mean_tpr_1A, color='b', lw=2, label=f'Mean ROC for Run 1A (AUC = {average_performance_1A:.2f})')\n",
    "plt.fill_between(mean_fprs_1A, mean_tpr_1A - std_tpr_1A, mean_tpr_1A + std_tpr_1A, color='grey', alpha=0.15)\n",
    "\n",
    "plt.plot(mean_fprs_2A4, mean_tpr_2A4, color='g', lw=2, label=f'Mean ROC for Run 2A v.4 (AUC = {average_performance_2A4:.2f})')\n",
    "plt.fill_between(mean_fprs_2A4, mean_tpr_2A4 - std_tpr_2A4, mean_tpr_2A4 + std_tpr_2A4, color='grey', alpha=0.15)\n",
    "\n",
    "plt.plot(mean_fprs_2A7, mean_tpr_2A7, color='r', lw=2, label=f'Mean ROC for Run 2A v.7 (AUC = {average_performance_2A7:.2f})')\n",
    "plt.fill_between(mean_fprs_2A7, mean_tpr_2A7 - std_tpr_2A7, mean_tpr_2A7 + std_tpr_2A7, color='grey', alpha=0.15)\n",
    "\n",
    "plt.plot([], [], color='grey', alpha=0.2, linewidth=10, label='±1 SD')\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"roc_curve_rvedv.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost auroc with REPEATED (20x) nested cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df[inputs_1a]\n",
    "y = df['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "n_repeats_outer = 20\n",
    "\n",
    "# make RepeatedStratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=n_splits_outer, n_repeats=n_repeats_outer, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "#create list to store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    #get feature importance for fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(fi)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig('1a_auroc_graph.svg', format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean and std dev of feature importances \n",
    "\n",
    "col0 = []\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "col4 = []\n",
    "col5 = []\n",
    "col6 = []\n",
    "col7 = []\n",
    "col8 = []\n",
    "\n",
    "\n",
    "\n",
    "for f in feature_importances:\n",
    "    \n",
    "    count = 0\n",
    "    for i in f:\n",
    "        if count == 0:\n",
    "            col0.append(i)\n",
    "        if count == 1:\n",
    "            col1.append(i)\n",
    "        if count == 2:\n",
    "            col2.append(i)\n",
    "        if count == 3:\n",
    "            col3.append(i)\n",
    "        if count == 4:\n",
    "            col4.append(i)\n",
    "        if count == 5:\n",
    "            col5.append(i)\n",
    "        if count == 6:\n",
    "            col6.append(i)\n",
    "        if count == 7:\n",
    "            col7.append(i)\n",
    "        if count == 8:\n",
    "            col8.append(i)\n",
    "            \n",
    "        count = count + 1\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(col8))\n",
    "print(np.std(col8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bar graph plot for aurocs\n",
    "# AUROC scores and standard deviations\n",
    "auroc_scores = [average_performance_1A, average_performance_2A4, average_performance_2A7]\n",
    "std_devs = [std_dev_performance_1A, std_dev_performance_2A4, std_dev_performance_2A7]\n",
    "\n",
    "# X-axis labels\n",
    "labels = ['Model 1', 'Model 2A v.4', 'Model 2A v.7']\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(labels, auroc_scores, yerr=std_devs, capsize=5, color='skyblue')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.ylabel('AUROC Score')\n",
    "plt.title('AUROC Scores with Standard Deviation')\n",
    "plt.ylim([0, 1])  # Optional: Ensuring y-axis starts at 0 and ends at 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Optional: Adding a grid for readability\n",
    "\n",
    "plt.savefig(\"roc_barplot_rvedv.svg\", format='svg')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get feature importances for each fold\n",
    "\n",
    "#print(feature_importances)\n",
    "\n",
    "for i in X_train.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get feature importance\n",
    "\n",
    "feature_importance = best_xgb_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance, tick_label=feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('XGBoost Classifier Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(feature_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## xgboost PR curves\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# setup\n",
    "X = df[inputs_1a]\n",
    "y = df['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='average_precision',  # making avg precision our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "#plt.figure(figsize=(8, 6))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# do stratification by outcome\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    #  probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # do PR curve vals\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the PR curve for the current cross-validation fold\n",
    "    ax.plot(recalls_list[fold], precisions_list[fold], lw=1, label=f'Fold {fold+1} (AUCPR = {round(auc_pr, 3)})')\n",
    "\n",
    "    # Add AUC-PR to the scores list\n",
    "    scores.append(auc_pr)\n",
    "\n",
    "# calculate average AUC-PR score across all stratified folds\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auprc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUPRC: {auprc:.4f}\")\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "\n",
    "\n",
    "\n",
    "# calculate the mean PR curve - must do sorting and interpolation to fit data correctly\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# plot  mean PR curve\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# plot the chance PR curve (should be a diagonal line from 0,1 to 1,0)\n",
    "#plt.plot([0, 1], [1, 0], color='gray', linestyle='--', lw=2, label='Chance')\n",
    "\n",
    "# Plot baseline prevalence as a horizontal line\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Other plot settings\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('AUPRC Curves for Stratified Cross-Validation Folds (Experiment 1A)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost pr curves with group strat + nested cv loops\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = merge_df[inputs_1a]\n",
    "y = merge_df['outcome_rvedvi_hi']\n",
    "groups = merge_df['mrn']  # Group identifier column\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# Number of splits for outer and inner cross-validation\n",
    "n_splits_outer = 5\n",
    "n_splits_inner = 3\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_group_kfold = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists to store results\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "average_aucpr_scores = []\n",
    "\n",
    "# Outer cross-validation loop\n",
    "for train_index, test_index in outer_group_kfold.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    groups_train = groups.iloc[train_index]\n",
    "\n",
    "    # XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # GroupKFold for inner cross-validation (hyperparameter tuning)\n",
    "    inner_group_kfold = GroupKFold(n_splits=n_splits_inner)\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='average_precision',\n",
    "        cv=inner_group_kfold.split(X_train, y_train, groups_train),\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # AUC-PR\n",
    "    auc_pr = auc(recall, precision)\n",
    "    average_aucpr_scores.append(auc_pr)\n",
    "\n",
    "# Calculate and print the average and standard deviation performance across all outer folds\n",
    "average_aucpr = np.mean(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", average_aucpr)\n",
    "std_aucpr = np.std(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "precision_interp = []\n",
    "\n",
    "# Interpolate and plot each fold's precision-recall curve\n",
    "for recall, precision in zip(recalls_list, precisions_list):\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recall, precision)))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "ax.fill_between(mean_recall, mean_precision - std_aucpr, mean_precision + std_aucpr, color='blue', alpha=0.2)\n",
    "plt.plot([], [], color='grey', alpha=0.2, linewidth=10, label='±1 SD')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curves for Group-Stratified Cross-Validation Folds (Experiment 1A)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 1A stuff\n",
    "\n",
    "mean_precision_1A = mean_precision\n",
    "mean_recall_1A = mean_recall\n",
    "average_aucpr_1A = average_aucpr\n",
    "std_aucpr_1A = std_aucpr\n",
    "baseline_prevalence_1A = baseline_prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlaying plots\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.plot(mean_recall_1A, mean_precision_1A, color='b', linestyle='-', lw=2, label=f'Mean AUPRC for Run 1A (AUC = {round(average_aucpr_1A, 3)})')\n",
    "\n",
    "ax.hlines(baseline_prevalence_1A, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence_1A, 3)})')\n",
    "\n",
    "ax.fill_between(mean_recall_1A, mean_precision_1A - std_aucpr_1A, mean_precision_1A + std_aucpr_1A, color='gray', alpha=0.15)\n",
    "\n",
    "\n",
    "ax.plot(mean_recall_2A4, mean_precision_2A4, color='g', linestyle='-', lw=2, label=f'Mean AUPRC for Run 2A v.4 (AUC = {round(average_aucpr_2A4, 3)})')\n",
    "\n",
    "#ax.hlines(baseline_prevalence_2A4, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence_2A4, 3)})')\n",
    "\n",
    "ax.fill_between(mean_recall_2A4, mean_precision_2A4 - std_aucpr_2A4, mean_precision_2A4 + std_aucpr_2A4, color='gray', alpha=0.15)\n",
    "\n",
    "\n",
    "ax.plot(mean_recall_2A7, mean_precision_2A7, color='r', linestyle='-', lw=2, label=f'Mean AUPRC for Run 2A v.7 (AUC = {round(average_aucpr_2A7, 3)})')\n",
    "\n",
    "#ax.hlines(baseline_prevalence_2A7, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence_2A7, 3)})')\n",
    "\n",
    "ax.fill_between(mean_recall_2A7, mean_precision_2A7 - std_aucpr_2A7, mean_precision_2A7 + std_aucpr_2A7, color='gray', alpha=0.15)\n",
    "\n",
    "\n",
    "plt.plot([], [], color='grey', alpha=0.2, linewidth=10, label='±1 SD')\n",
    "\n",
    "plt.ylim([0, 1])  # Setting y-axis limits from 0 to 1\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curves for Group-Stratified Cross-Validation Folds')\n",
    "\n",
    "plt.savefig(\"pr_curve_with_std.svg\", format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar plot for pr \n",
    "\n",
    "#make bar graph plot for aurocs\n",
    "# AUROC scores and standard deviations\n",
    "auroc_scores = [average_aucpr_1A, average_aucpr_2A4, average_aucpr_2A7]\n",
    "std_devs = [std_aucpr_1A, std_aucpr_2A4, std_aucpr_2A7]\n",
    "\n",
    "# X-axis labels\n",
    "labels = ['Model 1', 'Model 2A v.4', 'Model 2A v.7']\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(labels, auroc_scores, yerr=std_devs, capsize=5, color='skyblue')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.ylabel('AUPRC Score')\n",
    "plt.title('AUPRC Scores with Standard Deviation')\n",
    "plt.ylim([0, 1])  # Optional: Ensuring y-axis starts at 0 and ends at 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Optional: Adding a grid for readability\n",
    "\n",
    "plt.savefig(\"pr_barplot_rvedv.svg\", format='svg')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df[inputs_1a]\n",
    "y = df['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer cross-validation\n",
    "n_outer_splits = 5  # Number of outer splits\n",
    "outer_stratified_kfold = StratifiedKFold(n_splits=n_outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# Outer loop: iterate over outer folds\n",
    "for outer_fold, (train_index, test_index) in enumerate(outer_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {outer_fold+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "scores = [auc(recalls_list[i], precisions_list[i]) for i in range(len(recalls_list))]\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using REPEATED nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df[inputs_1a]\n",
    "y = df['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits and repeats for outer cross-validation\n",
    "n_splits_outer = 5  # Number of outer splits\n",
    "n_repeats_outer = 20  # Number of repeats\n",
    "\n",
    "# make RepeatedStratifiedKFold cross-validation iterator for outer cross-validation\n",
    "outer_repeated_stratified_kfold = RepeatedStratifiedKFold(n_splits=n_splits_outer, n_repeats=n_repeats_outer, random_state=42)\n",
    "\n",
    "# Lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "outer_fold_auprcs = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for i, (train_index, test_index) in enumerate(outer_repeated_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # add auc_pr to list\n",
    "    outer_fold_auprcs.append(auc_pr)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {i+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "average_aucpr = np.mean(outer_fold_auprcs)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(outer_fold_auprcs)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean PR curve\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('1a_auprc_graph.svg', format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Experiment 1B ########\n",
    "inputs_1b = inputs_1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1b = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1b = df_1b.dropna(subset=['outcome_rvesv_hi'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc curves \n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# setup\n",
    "X = df_1b[inputs_1b]\n",
    "y = df_1b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store fpr and tpr for each fold\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='roc_auc',  # making AUROC our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# do stratification by outcome\n",
    "\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    auroc_fold_scores = []\n",
    "\n",
    "    # Fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate AUROC for the current fold\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Add to AUROC, TPR, FPR, and thresholds fold scores\n",
    "    auroc_fold_scores.append(auroc)\n",
    "\n",
    "    # Compute ROC curve for the current fold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # Append fpr and tpr to the lists\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    # Plot the ROC curve for the current cross-validation fold\n",
    "    plt.step(fprs[fold], tprs[fold], lw=1, label=f'Fold {fold+1} (AUC = {round(auroc, 3)})')\n",
    "\n",
    "    # Add to AUROC list\n",
    "    scores.append(auroc)\n",
    "    \n",
    "# calculate average + std AUROC score across all stratified folds\n",
    "average_auroc = np.mean(scores)\n",
    "print(\"Average AUROC Across Stratified Folds:\", average_auroc)\n",
    "\n",
    "std_auroc = np.std(scores)\n",
    "print(\"Standard Deviation AUROC Across Stratified Folds:\", std_auroc)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auroc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# Calculate the mean ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)  # Choose the number of thresholds, here chose 100\n",
    "tpr_interp = []\n",
    "\n",
    "for fold in range(len(fprs)):\n",
    "    tpr_interp.append(np.interp(mean_fpr, fprs[fold], tprs[fold]))\n",
    "\n",
    "mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "\n",
    "# plot the average ROC curve and set labels\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', linestyle='-', lw=2, label=f'Mean ROC (AUC = {round(average_auroc,3)})')\n",
    "\n",
    "# other plot settings\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Stratified Cross-Validation Folds (Experiment 1B)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost auroc with nested cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_1b[inputs_1b]\n",
    "y = df_1b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## xgboost PR curves\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# setup\n",
    "X = df_1b[inputs_1b]\n",
    "y = df_1b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='average_precision',  # making avg precision our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "#plt.figure(figsize=(8, 6))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# do stratification by outcome\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    #  probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # do PR curve vals\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the PR curve for the current cross-validation fold\n",
    "    ax.plot(recalls_list[fold], precisions_list[fold], lw=1, label=f'Fold {fold+1} (AUCPR = {round(auc_pr, 3)})')\n",
    "\n",
    "    # Add AUC-PR to the scores list\n",
    "    scores.append(auc_pr)\n",
    "\n",
    "# calculate average AUC-PR score across all stratified folds\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auprc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUPRC: {auprc:.4f}\")\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "\n",
    "\n",
    "\n",
    "# calculate the mean PR curve - must do sorting and interpolation to fit data correctly\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# plot  mean PR curve\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# plot the chance PR curve (should be a diagonal line from 0,1 to 1,0)\n",
    "#plt.plot([0, 1], [1, 0], color='gray', linestyle='--', lw=2, label='Chance')\n",
    "\n",
    "# Plot baseline prevalence as a horizontal line\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Other plot settings\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('AUPRC Curves for Stratified Cross-Validation Folds (Experiment 1B)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df_1b[inputs_1b]\n",
    "y = df_1b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer cross-validation\n",
    "n_outer_splits = 5  # Number of outer splits\n",
    "outer_stratified_kfold = StratifiedKFold(n_splits=n_outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# Outer loop: iterate over outer folds\n",
    "for outer_fold, (train_index, test_index) in enumerate(outer_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {outer_fold+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "scores = [auc(recalls_list[i], precisions_list[i]) for i in range(len(recalls_list))]\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Experiment 2A #####\n",
    "\n",
    "inputs_2a = ['age_y_at_echo',\n",
    "'sex',\n",
    "'pulmonaryoutflowtypeattimeo_2', 'pulmonaryoutflowtypeattimeo_1', 'pulmonaryoutflowtypeattimeo_5', 'pulmonaryoutflowtypeattimeo_3', 'pulmonaryoutflowtypeattimeo_8', 'pulmonaryoutflowtypeattimeo_7', 'pulmonaryoutflowtypeattimeo_6',\n",
    "'degreeofprbyechoreportif_1', 'degreeofprbyechoreportif_2', 'degreeofprbyechoreportif_3', 'degreeofprbyechoreportif_4', 'degreeofprbyechoreportif_5',\n",
    "'degreeoftrbyechoreportif_0', 'degreeoftrbyechoreportif_1', 'degreeoftrbyechoreportif_2', 'degreeoftrbyechoreportif_3', 'degreeoftrbyechoreportif_4', 'degreeoftrbyechoreportif_5', 'degreeoftrbyechoreportif_6',\n",
    "'tapsecmap4cviewmmode',\n",
    "'rvs1tdicmsap4cviewtd',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'mvannulusdimensioncmap4c',\n",
    "'rvenddiastolicareacm2ap',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvmidenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvendsystolicareacm2ap4',\n",
    "'aovalvesystolicannulardimens',\n",
    "'pslaxpulmvalveannulusindias',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'pssapulmvalveannulusinsysto',\n",
    "'mpadiastolicflowreversalpres',\n",
    "'branchpaatleastonediastol',\n",
    "'antegradediastolicflowateith',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'tvannulusdimensionindiastole',\n",
    "'weightonmri',\n",
    "'heightonmri',\n",
    "'bsaonmri',\n",
    "'rv_fac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs_2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_new_run2 = ['age_y_at_echo',\n",
    "'degreeofprbyechoreportif_5',\n",
    "'degreeoftrbyechoreportif_0',\n",
    "'tapsecmap4cviewmmode',\n",
    "'rvs1tdicmsap4cviewtd',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'mvannulusdimensioncmap4c',\n",
    "'rvenddiastolicareacm2ap',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvmidenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvendsystolicareacm2ap4',\n",
    "'aovalvesystolicannulardimens',\n",
    "'pslaxpulmvalveannulusindias',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'pssapulmvalveannulusinsysto',\n",
    "'branchpaatleastonediastol',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'tvannulusdimensionindiastole',\n",
    "'weightonmri',\n",
    "'heightonmri',\n",
    "'rv_fac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_new_run3 = ['weightonmri',\n",
    "'heightonmri',\n",
    "'rvendsystolicareacm2ap4',\n",
    "'aovalvesystolicannulardimens',\n",
    "'mvannulusdimensioncmap4c',\n",
    "'tvannulusdimensionindiastole',\n",
    "'rvs1tdicmsap4cviewtd',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'rvmidenddiastolicdimension',\n",
    "'tapsecmap4cviewmmode',\n",
    "'rvotproximal12oclockdiast',\n",
    "'age_y_at_echo',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'degreeofprbyechoreportif_5',\n",
    "'rvenddiastolicareacm2ap',\n",
    "'pslarvotdiastolicdimensionc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_new_run4 = ['heightonmri',\n",
    "'tvannulusdimensionindiastole',\n",
    "'age_y_at_echo',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'tapsecmap4cviewmmode',\n",
    "'degreeofprbyechoreportif_5',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'rvenddiastolicareacm2ap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_new_run5 = ['heightonmri',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'degreeofprbyechoreportif_5',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvotproximal12oclockdiast',\n",
    "'tapsecmap4cviewmmode',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'rvenddiastolicareacm2ap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_new_run6 = ['heightonmri',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvotproximal12oclockdiast',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'rvenddiastolicareacm2ap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_new_run7 = ['heightonmri',\n",
    "'tvannulusdimensionindiastole',\n",
    "'age_y_at_echo',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'tapsecmap4cviewmmode',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'rvenddiastolicareacm2ap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_run1 = ['age_y_at_echo',\n",
    "'degreeofprbyechoreportif_1',\n",
    "'degreeofprbyechoreportif_2',\n",
    "'degreeofprbyechoreportif_3',\n",
    "'degreeofprbyechoreportif_4',\n",
    "'degreeofprbyechoreportif_5',\n",
    "'tr_greater_than_mild',\n",
    "'new_pulm_outflow_col_1',\n",
    "'new_pulm_outflow_col_2',\n",
    "'new_pulm_outflow_col_3',\n",
    "'tapsecmap4cviewmmode',\n",
    "'rvs1tdicmsap4cviewtd',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'mvannulusdimensioncmap4c',\n",
    "'rvenddiastolicareacm2ap',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvmidenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvendsystolicareacm2ap4',\n",
    "'aovalvesystolicannulardimens',\n",
    "'pslaxpulmvalveannulusindias',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'pssapulmvalveannulusinsysto',\n",
    "'mpadiastolicflowreversalpres',\n",
    "'branchpaatleastonediastol',\n",
    "'antegradediastolicflowateith',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'tvannulusdimensionindiastole',\n",
    "'weightonmri',\n",
    "'heightonmri',\n",
    "'bsaonmri',\n",
    "'rv_fac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_2 = ['age_y_at_echo',\n",
    "'rvenddiastolicareacm2ap',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'pssapulmvalveannulusinsysto',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'tvannulusdimensionindiastole',\n",
    "'weightonmri',\n",
    "'heightonmri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_run4 = ['age_y_at_echo',\n",
    "'degreeofprbyechoreportif_5',\n",
    "'rvenddiastolicareacm2ap',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'pssapulmvalveannulusinsysto',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'tvannulusdimensionindiastole',\n",
    "'weightonmri',\n",
    "'heightonmri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2a_run5 = ['age_y_at_echo',\n",
    "'rvenddiastolicareacm2ap',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'rvotproximal12oclockdiast',\n",
    "'rvotdistaljustbelowpulmvlv',\n",
    "'pssapulmvalveannulusinsysto',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'tvannulusdimensionindiastole',\n",
    "'weightonmri',\n",
    "'heightonmri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding\n",
    "df_encoded = pd.get_dummies(merge_df, columns=['pulmonaryoutflowtypeattimeo', 'degreeofprbyechoreportif', 'degreeoftrbyechoreportif'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding - to updated dataset (df_2)\n",
    "df_encoded_2 = pd.get_dummies(df_2, columns=['new_pulm_outflow_col', 'degreeofprbyechoreportif'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_2['outcome_rvedvi_hi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_encoded_2['mrn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc curves \n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# setup\n",
    "X = df_encoded[inputs_2a]\n",
    "y = df_encoded['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store fpr and tpr for each fold\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='roc_auc',  # making AUROC our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# do stratification by outcome\n",
    "\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    auroc_fold_scores = []\n",
    "\n",
    "    # Fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate AUROC for the current fold\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Add to AUROC, TPR, FPR, and thresholds fold scores\n",
    "    auroc_fold_scores.append(auroc)\n",
    "\n",
    "    # Compute ROC curve for the current fold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # Append fpr and tpr to the lists\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    # Plot the ROC curve for the current cross-validation fold\n",
    "    plt.step(fprs[fold], tprs[fold], lw=1, label=f'Fold {fold+1} (AUC = {round(auroc, 3)})')\n",
    "\n",
    "    # Add to AUROC list\n",
    "    scores.append(auroc)\n",
    "    \n",
    "# calculate average + std AUROC score across all stratified folds\n",
    "average_auroc = np.mean(scores)\n",
    "print(\"Average AUROC Across Stratified Folds:\", average_auroc)\n",
    "\n",
    "std_auroc = np.std(scores)\n",
    "print(\"Standard Deviation AUROC Across Stratified Folds:\", std_auroc)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auroc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# Calculate the mean ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)  # Choose the number of thresholds, here chose 100\n",
    "tpr_interp = []\n",
    "\n",
    "for fold in range(len(fprs)):\n",
    "    tpr_interp.append(np.interp(mean_fpr, fprs[fold], tprs[fold]))\n",
    "\n",
    "mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "\n",
    "# plot the average ROC curve and set labels\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', linestyle='-', lw=2, label=f'Mean ROC (AUC = {round(average_auroc,3)})')\n",
    "\n",
    "# other plot settings\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Stratified Cross-Validation Folds (Experiment 2A)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc with group stratification + nested cv\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df_encoded[inputs_2a_new_run4]\n",
    "y = df_encoded['outcome_rvedvi_hi']\n",
    "groups = df_encoded['mrn']  # Group identifier\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Number of splits for cross-validation\n",
    "n_splits_outer = 5\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_cv = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists for storing results\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)\n",
    "tpr_interpolations = []\n",
    "\n",
    "#save feature importances per fold\n",
    "feature_importances = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,  # You can adjust this value\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    #get feature importance for fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(fi)\n",
    "\n",
    "# Average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', lw=2, label=f'Mean ROC (AUC = {average_performance:.2f})')\n",
    "plt.fill_between(mean_fprs, mean_tpr - std_tpr, mean_tpr + std_tpr, color='grey', alpha=0.2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given values\n",
    "prevalence = 0.0905\n",
    "sensitivity = 0.90\n",
    "specificity = 0.727\n",
    "\n",
    "# Calculate PPV\n",
    "ppv = (sensitivity * prevalence) / (sensitivity * prevalence + (1 - specificity) * (1 - prevalence))\n",
    "\n",
    "# Calculate NPV\n",
    "npv = (specificity * (1 - prevalence)) / ((1 - sensitivity) * prevalence + specificity * (1 - prevalence))\n",
    "\n",
    "# Print results\n",
    "print(\"Positive Predictive Value (PPV):\", ppv)\n",
    "print(\"Negative Predictive Value (NPV):\", npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['outcome_rvedvi_hi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_encoded[df_encoded['outcome_rvedvi_hi'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all 2A run 4 stuff\n",
    "average_performance_2A4 = average_performance\n",
    "std_dev_performance_2A4 = std_dev_performance\n",
    "mean_tpr_2A4 = mean_tpr\n",
    "std_tpr_2A4 = std_tpr\n",
    "mean_fprs_2A4 = mean_fprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all 2A run 7 stuff\n",
    "average_performance_2A7 = average_performance\n",
    "std_dev_performance_2A7 = std_dev_performance\n",
    "mean_tpr_2A7 = mean_tpr\n",
    "std_tpr_2A7 = std_tpr\n",
    "mean_fprs_2A7 = mean_fprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost auroc with nested cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_encoded_2[inputs_2a_run4]\n",
    "y = df_encoded_2['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "#create list to store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=3000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    #get feature importance for fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(fi)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost auroc with REPEATED (20x) nested cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_encoded_2[inputs_2a_run5]\n",
    "y = df_encoded_2['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "n_repeats_outer = 20\n",
    "\n",
    "# make RepeatedStratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=n_splits_outer, n_repeats=n_repeats_outer, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "#create list to store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    #get feature importance for fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(fi)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves with shading for standard deviation\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "#plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "#plt.title('Mean ROC Curve with Standard Deviation')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig('2a_run5_auroc_graph.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_importances_2arun4 = [0.0145127644,\n",
    "0.03632373,\n",
    "0.0494950644,\n",
    "0.0573905092,\n",
    "0.081441497,\n",
    "0.09095655,\n",
    "0.094159966,\n",
    "0.0947930042,\n",
    "0.104949613,\n",
    "0.110177038,\n",
    "0.1317176888,\n",
    "0.134082564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in feature_importances:\n",
    "    for i in f:\n",
    "        print(i)\n",
    "        \n",
    "    print(\" \")\n",
    "    \n",
    "    \n",
    "for i in X_train.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_features_1a = [0.227449248,\n",
    "0.08479753,\n",
    "0.017493238,\n",
    "0.262206272,\n",
    "0.034100496,\n",
    "0.2407132284,\n",
    "0.0378129708,\n",
    "0.035912066,\n",
    "0.0595149368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_features_3 = [0.078271715,\n",
    "0.143294996,\n",
    "0.134644461,\n",
    "0.360289106,\n",
    "0.0667977328,\n",
    "0.216702006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean and std dev of feature importances \n",
    "\n",
    "col0 = []\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "col4 = []\n",
    "col5 = []\n",
    "col6 = []\n",
    "col7 = []\n",
    "col8 = []\n",
    "col9 = []\n",
    "col10 = []\n",
    "\n",
    "\n",
    "for f in feature_importances:\n",
    "    \n",
    "    count = 0\n",
    "    for i in f:\n",
    "        if count == 0:\n",
    "            col0.append(i)\n",
    "        if count == 1:\n",
    "            col1.append(i)\n",
    "        if count == 2:\n",
    "            col2.append(i)\n",
    "        if count == 3:\n",
    "            col3.append(i)\n",
    "        if count == 4:\n",
    "            col4.append(i)\n",
    "        if count == 5:\n",
    "            col5.append(i)\n",
    "        if count == 6:\n",
    "            col6.append(i)\n",
    "        if count == 7:\n",
    "            col7.append(i)\n",
    "        if count == 8:\n",
    "            col8.append(i)\n",
    "        if count == 9:\n",
    "            col9.append(i)\n",
    "        if count == 10:\n",
    "            col10.append(i)\n",
    "            \n",
    "        count = count + 1\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(col10))\n",
    "print(np.std(col10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_2arun4 = ['tvannulusdimensionindiastole',\n",
    "'heightonmri',\n",
    "'age_y_at_echo',\n",
    "'tvangulartiltdegreesap4c',\n",
    "'pssapulmvalveannulusindiast',\n",
    "'rvlengthenddiastolicdimensio',\n",
    "'degreeofprbyechoreportif_5',\n",
    "'rvbasalenddiastolicdimension',\n",
    "'rvotproximal12oclockdiast',\n",
    "'tapsecmap4cviewmmode',\n",
    "'pslarvotdiastolicdimensionc',\n",
    "'rvenddiastolicareacm2ap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get feature importance\n",
    "\n",
    "feature_importance = avg_features_3\n",
    "feature_names = X_train.columns\n",
    "\n",
    "\n",
    "# Assuming feature_importance and feature_names are defined as in your code\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Set custom tick positions\n",
    "tick_positions = np.arange(len(feature_importance)) * 1.5  # Adjust the multiplier to increase spacing\n",
    "\n",
    "plt.barh(tick_positions, feature_importance, tick_label=feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Average Feature Importance Experiment 3')\n",
    "plt.yticks(tick_positions, feature_names)  # Apply custom tick positions\n",
    "\n",
    "plt.savefig(\"avg_feature_performance_3.svg\", format='svg')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## xgboost PR curves\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# setup\n",
    "X = df_encoded[inputs_2a]\n",
    "y = df_encoded['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='average_precision',  # making avg precision our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "#plt.figure(figsize=(8, 6))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# do stratification by outcome\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    #  probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # do PR curve vals\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the PR curve for the current cross-validation fold\n",
    "    ax.plot(recalls_list[fold], precisions_list[fold], lw=1, label=f'Fold {fold+1} (AUCPR = {round(auc_pr, 3)})')\n",
    "\n",
    "    # Add AUC-PR to the scores list\n",
    "    scores.append(auc_pr)\n",
    "\n",
    "# calculate average AUC-PR score across all stratified folds\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auprc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUPRC: {auprc:.4f}\")\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "\n",
    "\n",
    "\n",
    "# calculate the mean PR curve - must do sorting and interpolation to fit data correctly\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# plot  mean PR curve\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# plot the chance PR curve (should be a diagonal line from 0,1 to 1,0)\n",
    "#plt.plot([0, 1], [1, 0], color='gray', linestyle='--', lw=2, label='Chance')\n",
    "\n",
    "# Plot baseline prevalence as a horizontal line\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Other plot settings\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('AUPRC Curves for Stratified Cross-Validation Folds (Experiment 2A)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost pr curves with group strat + nested cv loops\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_encoded[inputs_2a_new_run7]\n",
    "y = df_encoded['outcome_rvedvi_hi']\n",
    "groups = df_encoded['mrn']  # Group identifier column\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# Number of splits for outer and inner cross-validation\n",
    "n_splits_outer = 5\n",
    "n_splits_inner = 3\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_group_kfold = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists to store results\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "average_aucpr_scores = []\n",
    "\n",
    "# Outer cross-validation loop\n",
    "for train_index, test_index in outer_group_kfold.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    groups_train = groups.iloc[train_index]\n",
    "\n",
    "    # XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # GroupKFold for inner cross-validation (hyperparameter tuning)\n",
    "    inner_group_kfold = GroupKFold(n_splits=n_splits_inner)\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='average_precision',\n",
    "        cv=inner_group_kfold.split(X_train, y_train, groups_train),\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # AUC-PR\n",
    "    auc_pr = auc(recall, precision)\n",
    "    average_aucpr_scores.append(auc_pr)\n",
    "\n",
    "# Calculate and print the average and standard deviation performance across all outer folds\n",
    "average_aucpr = np.mean(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", average_aucpr)\n",
    "std_aucpr = np.std(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "precision_interp = []\n",
    "\n",
    "# Interpolate and plot each fold's precision-recall curve\n",
    "for recall, precision in zip(recalls_list, precisions_list):\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recall, precision)))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curves for Group-Stratified Cross-Validation Folds (Experiment 1A)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 2A run 4 stuff\n",
    "\n",
    "mean_precision_2A4 = mean_precision\n",
    "mean_recall_2A4 = mean_recall\n",
    "average_aucpr_2A4 = average_aucpr\n",
    "std_aucpr_2A4 = std_aucpr\n",
    "baseline_prevalence_2A4 = baseline_prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_precision_2A7 = mean_precision\n",
    "mean_recall_2A7 = mean_recall\n",
    "average_aucpr_2A7 = average_aucpr\n",
    "std_aucpr_2A7 = std_aucpr\n",
    "baseline_prevalence_2A7 = baseline_prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df_encoded_2[inputs_2a_run4]\n",
    "y = df_encoded_2['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer cross-validation\n",
    "n_outer_splits = 5  # Number of outer splits\n",
    "outer_stratified_kfold = StratifiedKFold(n_splits=n_outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# Outer loop: iterate over outer folds\n",
    "for outer_fold, (train_index, test_index) in enumerate(outer_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=3000,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {outer_fold+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "scores = [auc(recalls_list[i], precisions_list[i]) for i in range(len(recalls_list))]\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using REPEATED nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df_encoded_2[inputs_2a_run4]\n",
    "y = df_encoded_2['outcome_rvedvi_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits and repeats for outer cross-validation\n",
    "n_splits_outer = 5  # Number of outer splits\n",
    "n_repeats_outer = 20  # Number of repeats\n",
    "\n",
    "# make RepeatedStratifiedKFold cross-validation iterator for outer cross-validation\n",
    "outer_repeated_stratified_kfold = RepeatedStratifiedKFold(n_splits=n_splits_outer, n_repeats=n_repeats_outer, random_state=42)\n",
    "\n",
    "# Lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "outer_fold_auprcs = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for i, (train_index, test_index) in enumerate(outer_repeated_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # add auc_pr to list\n",
    "    outer_fold_auprcs.append(auc_pr)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {i+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "average_aucpr = np.mean(outer_fold_auprcs)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(outer_fold_auprcs)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('2a_run4_auprc_graph.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 2B ####\n",
    "\n",
    "inputs_2b = inputs_2a\n",
    "\n",
    "df_2b = df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2b = df_2b.dropna(subset=['outcome_rvesv_hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc curves \n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# setup\n",
    "X = df_2b[inputs_2b]\n",
    "y = df_2b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store fpr and tpr for each fold\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='roc_auc',  # making AUROC our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# do stratification by outcome\n",
    "\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    auroc_fold_scores = []\n",
    "\n",
    "    # Fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate AUROC for the current fold\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Add to AUROC, TPR, FPR, and thresholds fold scores\n",
    "    auroc_fold_scores.append(auroc)\n",
    "\n",
    "    # Compute ROC curve for the current fold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # Append fpr and tpr to the lists\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    # Plot the ROC curve for the current cross-validation fold\n",
    "    plt.step(fprs[fold], tprs[fold], lw=1, label=f'Fold {fold+1} (AUC = {round(auroc, 3)})')\n",
    "\n",
    "    # Add to AUROC list\n",
    "    scores.append(auroc)\n",
    "    \n",
    "# calculate average + std AUROC score across all stratified folds\n",
    "average_auroc = np.mean(scores)\n",
    "print(\"Average AUROC Across Stratified Folds:\", average_auroc)\n",
    "\n",
    "std_auroc = np.std(scores)\n",
    "print(\"Standard Deviation AUROC Across Stratified Folds:\", std_auroc)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auroc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# Calculate the mean ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)  # Choose the number of thresholds, here chose 100\n",
    "tpr_interp = []\n",
    "\n",
    "for fold in range(len(fprs)):\n",
    "    tpr_interp.append(np.interp(mean_fpr, fprs[fold], tprs[fold]))\n",
    "\n",
    "mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "\n",
    "# plot the average ROC curve and set labels\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', linestyle='-', lw=2, label=f'Mean ROC (AUC = {round(average_auroc,3)})')\n",
    "\n",
    "# other plot settings\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Stratified Cross-Validation Folds (Experiment 2B)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost auroc with nested cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_2b[inputs_2b]\n",
    "y = df_2b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## xgboost PR curves\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# setup\n",
    "X = df_2b[inputs_2b]\n",
    "y = df_2b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='average_precision',  # making avg precision our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "#plt.figure(figsize=(8, 6))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# do stratification by outcome\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    #  probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # do PR curve vals\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the PR curve for the current cross-validation fold\n",
    "    ax.plot(recalls_list[fold], precisions_list[fold], lw=1, label=f'Fold {fold+1} (AUCPR = {round(auc_pr, 3)})')\n",
    "\n",
    "    # Add AUC-PR to the scores list\n",
    "    scores.append(auc_pr)\n",
    "\n",
    "# calculate average AUC-PR score across all stratified folds\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auprc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUPRC: {auprc:.4f}\")\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "\n",
    "\n",
    "\n",
    "# calculate the mean PR curve - must do sorting and interpolation to fit data correctly\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# plot  mean PR curve\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# plot the chance PR curve (should be a diagonal line from 0,1 to 1,0)\n",
    "#plt.plot([0, 1], [1, 0], color='gray', linestyle='--', lw=2, label='Chance')\n",
    "\n",
    "# Plot baseline prevalence as a horizontal line\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Other plot settings\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('AUPRC Curves for Stratified Cross-Validation Folds (Experiment 2B)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df_2b[inputs_2b]\n",
    "y = df_2b['outcome_rvesv_hi']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer cross-validation\n",
    "n_outer_splits = 5  # Number of outer splits\n",
    "outer_stratified_kfold = StratifiedKFold(n_splits=n_outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# Outer loop: iterate over outer folds\n",
    "for outer_fold, (train_index, test_index) in enumerate(outer_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {outer_fold+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "scores = [auc(recalls_list[i], precisions_list[i]) for i in range(len(recalls_list))]\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Experiment 3 #####\n",
    "inputs_3 = ['tapsecmap4cviewmmode','rvs1tdicmsap4cviewtd',\n",
    "           'rvenddiastolicareacm2ap', 'rvendsystolicareacm2ap4',\n",
    "           'bsaonmri', 'rv_fac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_3.dropna(subset=['outcome_rvef_low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc curves \n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# setup\n",
    "X = df_3[inputs_3]\n",
    "y = df_3['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store fpr and tpr for each fold\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='roc_auc',  # making AUROC our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "#store feature importances for each fold\n",
    "feature_importances = []\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# do stratification by outcome\n",
    "\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    auroc_fold_scores = []\n",
    "\n",
    "    # Fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate AUROC for the current fold\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Add to AUROC, TPR, FPR, and thresholds fold scores\n",
    "    auroc_fold_scores.append(auroc)\n",
    "\n",
    "    # Compute ROC curve for the current fold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # Append fpr and tpr to the lists\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    # Plot the ROC curve for the current cross-validation fold\n",
    "    plt.step(fprs[fold], tprs[fold], lw=1, label=f'Fold {fold+1} (AUC = {round(auroc, 3)})')\n",
    "\n",
    "    # Add to AUROC list\n",
    "    scores.append(auroc)\n",
    "    \n",
    "    #get feature importance for fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(fi)\n",
    "    \n",
    "# calculate average + std AUROC score across all stratified folds\n",
    "average_auroc = np.mean(scores)\n",
    "print(\"Average AUROC Across Stratified Folds:\", average_auroc)\n",
    "\n",
    "std_auroc = np.std(scores)\n",
    "print(\"Standard Deviation AUROC Across Stratified Folds:\", std_auroc)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auroc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# Calculate the mean ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)  # Choose the number of thresholds, here chose 100\n",
    "tpr_interp = []\n",
    "\n",
    "for fold in range(len(fprs)):\n",
    "    tpr_interp.append(np.interp(mean_fpr, fprs[fold], tprs[fold]))\n",
    "\n",
    "mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "\n",
    "# plot the average ROC curve and set labels\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', linestyle='-', lw=2, label=f'Mean ROC (AUC = {round(average_auroc,3)})')\n",
    "\n",
    "# other plot settings\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Stratified Cross-Validation Folds (Experiment 3)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc with group stratification and nested cv\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df_3[inputs_3]\n",
    "y = df_3['outcome_rvef_low']\n",
    "groups = df_3['mrn']  # Group identifier\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Number of splits for cross-validation\n",
    "n_splits_outer = 5\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_cv = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists for storing results\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)\n",
    "tpr_interpolations = []\n",
    "\n",
    "#store feature importances per fold\n",
    "feature_importances = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,  # You can adjust this value\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    #get feature importance for fold\n",
    "    fi = best_model.feature_importances_\n",
    "    sum_fi = sum(fi)\n",
    "    temp_fi = [x / sum_fi for x in fi]\n",
    "    feature_importances.append(fi)\n",
    "\n",
    "# Average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "#mean_tpr[-1] = 1.0\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', lw=2, label=f'Mean ROC (AUC = {average_performance:.2f})')\n",
    "plt.fill_between(mean_fprs, mean_tpr - std_tpr, mean_tpr + std_tpr, color='gray', alpha=0.15)\n",
    "\n",
    "plt.plot([], [], color='grey', alpha=0.2, linewidth=10, label='±1 SD')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"roc_curve_with_std_3.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fprs3 = mean_fprs\n",
    "mean_tpr3 = mean_tpr\n",
    "average_performance3 = average_performance\n",
    "std_tpr3 = std_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves - overlayed plot with exp 4\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(mean_fprs3, mean_tpr3, color='b', lw=2, label=f'Mean ROC for ASE Guidelines (AUC = {average_performance3:.2f})')\n",
    "plt.fill_between(mean_fprs3, mean_tpr3 - std_tpr3, mean_tpr3 + std_tpr3, color='gray', alpha=0.15)\n",
    "\n",
    "plt.plot(mean_fprs, mean_tpr, color='g', lw=2, label=f'Mean ROC for Expanded Dataset (AUC = {average_performance:.2f})')\n",
    "plt.fill_between(mean_fprs, mean_tpr - std_tpr, mean_tpr + std_tpr, color='gray', alpha=0.15)\n",
    "\n",
    "plt.plot([], [], color='grey', alpha=0.2, linewidth=10, label='±1 SD')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"roc_curve_rvef_overlayed.svg\", format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recall3 = mean_recall\n",
    "mean_precision4 = mean_precision\n",
    "average_aucpr4 = average_aucpr\n",
    "baseline_prevalence4 = baseline_prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost auroc with nested cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_3[inputs_3]\n",
    "y = df_3['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get feature importance\n",
    "\n",
    "feature_importance = best_xgb_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance, tick_label=feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('XGBoost Classifier Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## xgboost PR curves\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# setup\n",
    "X = df_3[inputs_3]\n",
    "y = df_3['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='average_precision',  # making avg precision our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "#plt.figure(figsize=(8, 6))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# do stratification by outcome\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    #  probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # do PR curve vals\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the PR curve for the current cross-validation fold\n",
    "    ax.plot(recalls_list[fold], precisions_list[fold], lw=1, label=f'Fold {fold+1} (AUCPR = {round(auc_pr, 3)})')\n",
    "\n",
    "    # Add AUC-PR to the scores list\n",
    "    scores.append(auc_pr)\n",
    "\n",
    "# calculate average AUC-PR score across all stratified folds\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auprc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUPRC: {auprc:.4f}\")\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "\n",
    "\n",
    "\n",
    "# calculate the mean PR curve - must do sorting and interpolation to fit data correctly\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# plot  mean PR curve\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# plot the chance PR curve (should be a diagonal line from 0,1 to 1,0)\n",
    "#plt.plot([0, 1], [1, 0], color='gray', linestyle='--', lw=2, label='Chance')\n",
    "\n",
    "# Plot baseline prevalence as a horizontal line\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Other plot settings\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('AUPRC Curves for Stratified Cross-Validation Folds (Experiment 3)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost pr curves with group strat + nested cv loops\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_3[inputs_3]\n",
    "y = df_3['outcome_rvef_low']\n",
    "groups = df_3['mrn']  # Group identifier column\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# Number of splits for outer and inner cross-validation\n",
    "n_splits_outer = 5\n",
    "n_splits_inner = 3\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_group_kfold = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists to store results\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "average_aucpr_scores = []\n",
    "\n",
    "# Outer cross-validation loop\n",
    "for train_index, test_index in outer_group_kfold.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    groups_train = groups.iloc[train_index]\n",
    "\n",
    "    # XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # GroupKFold for inner cross-validation (hyperparameter tuning)\n",
    "    inner_group_kfold = GroupKFold(n_splits=n_splits_inner)\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='average_precision',\n",
    "        cv=inner_group_kfold.split(X_train, y_train, groups_train),\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # AUC-PR\n",
    "    auc_pr = auc(recall, precision)\n",
    "    average_aucpr_scores.append(auc_pr)\n",
    "\n",
    "# Calculate and print the average and standard deviation performance across all outer folds\n",
    "average_aucpr = np.mean(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", average_aucpr)\n",
    "std_aucpr = np.std(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "precision_interp = []\n",
    "\n",
    "# Interpolate and plot each fold's precision-recall curve\n",
    "for recall, precision in zip(recalls_list, precisions_list):\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recall, precision)))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "ax.fill_between(mean_recall, mean_precision - std_aucpr, mean_precision + std_aucpr, color='gray', alpha=0.15)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curves for Group-Stratified Cross-Validation Folds (Experiment 3)')\n",
    "plt.savefig(\"pr_curve_with_std_3.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "precision_interp = []\n",
    "\n",
    "# Interpolate and plot each fold's precision-recall curve\n",
    "for recall, precision in zip(recalls_list, precisions_list):\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recall, precision)))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "ax.fill_between(mean_recall, mean_precision - std_aucpr, mean_precision + std_aucpr, color='gray', alpha=0.15)\n",
    "plt.ylim([0, 1])  # Setting y-axis limits from 0 to 1\n",
    "\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curves for Group-Stratified Cross-Validation Folds (Experiment 3)')\n",
    "plt.savefig(\"pr_curve_with_std_3.svg\", format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df_3[inputs_3]\n",
    "y = df_3['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer cross-validation\n",
    "n_outer_splits = 5  # Number of outer splits\n",
    "outer_stratified_kfold = StratifiedKFold(n_splits=n_outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# Outer loop: iterate over outer folds\n",
    "for outer_fold, (train_index, test_index) in enumerate(outer_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {outer_fold+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "scores = [auc(recalls_list[i], precisions_list[i]) for i in range(len(recalls_list))]\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Experiment 4 #####\n",
    "\n",
    "inputs_4 = inputs_2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df_4.dropna(subset=['outcome_rvef_low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc curves \n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# setup\n",
    "X = df_4[inputs_4]\n",
    "y = df_4['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store fpr and tpr for each fold\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='roc_auc',  # making AUROC our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# do stratification by outcome\n",
    "\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    auroc_fold_scores = []\n",
    "\n",
    "    # Fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate AUROC for the current fold\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Add to AUROC, TPR, FPR, and thresholds fold scores\n",
    "    auroc_fold_scores.append(auroc)\n",
    "\n",
    "    # Compute ROC curve for the current fold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # Append fpr and tpr to the lists\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    # Plot the ROC curve for the current cross-validation fold\n",
    "    plt.step(fprs[fold], tprs[fold], lw=1, label=f'Fold {fold+1} (AUC = {round(auroc, 3)})')\n",
    "\n",
    "    # Add to AUROC list\n",
    "    scores.append(auroc)\n",
    "    \n",
    "# calculate average + std AUROC score across all stratified folds\n",
    "average_auroc = np.mean(scores)\n",
    "print(\"Average AUROC Across Stratified Folds:\", average_auroc)\n",
    "\n",
    "std_auroc = np.std(scores)\n",
    "print(\"Standard Deviation AUROC Across Stratified Folds:\", std_auroc)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auroc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# Calculate the mean ROC curve\n",
    "mean_fpr = np.linspace(0, 1, 100)  # Choose the number of thresholds, here chose 100\n",
    "tpr_interp = []\n",
    "\n",
    "for fold in range(len(fprs)):\n",
    "    tpr_interp.append(np.interp(mean_fpr, fprs[fold], tprs[fold]))\n",
    "\n",
    "mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "\n",
    "# plot the average ROC curve and set labels\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', linestyle='-', lw=2, label=f'Mean ROC (AUC = {round(average_auroc,3)})')\n",
    "\n",
    "# other plot settings\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Stratified Cross-Validation Folds (Experiment 4)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgboost auroc with group stratification and nested cv\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df_4[inputs_4]\n",
    "y = df_4['outcome_rvef_low']\n",
    "groups = df_4['mrn']  # Group identifier\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Number of splits for cross-validation\n",
    "n_splits_outer = 5\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_cv = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists for storing results\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)\n",
    "tpr_interpolations = []\n",
    "\n",
    "y_train_vals = []\n",
    "y_test_vals = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,  # You can adjust this value\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "    \n",
    "    y_train_vals.append(y_train)\n",
    "    y_test_vals.append(y_test)\n",
    "\n",
    "# Average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(mean_fprs, mean_tpr, color='g', lw=2, label=f'Mean ROC (AUC = {average_performance:.2f})')\n",
    "plt.fill_between(mean_fprs, mean_tpr - std_tpr, mean_tpr + std_tpr, color='grey', alpha=0.2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"roc_curve_4.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(mean_fprs, mean_tpr, color='g', lw=2, label=f'Mean ROC (AUC = {average_performance:.2f})')\n",
    "plt.fill_between(mean_fprs, mean_tpr - std_tpr, mean_tpr + std_tpr, color='grey', alpha=0.2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Add line representing chance\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='black', label='Chance')\n",
    "\n",
    "\n",
    "plt.savefig(\"roc_curve_4.svg\", format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_vals[1]\n",
    "\n",
    "temp=0\n",
    "for i in y_test_vals[4]:\n",
    "    temp += i\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost auroc with nested cross validation\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_4[inputs_4]\n",
    "y = df_4['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'reg_alpha': [0.0, 1.0, 10.0],\n",
    "    'reg_lambda': [0.0, 1.0, 10.0],\n",
    "    'gamma': [0, 1.0, 5.0],\n",
    "    'min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the number of splits for both inner and outer cross-validation\n",
    "n_splits_inner = 3\n",
    "n_splits_outer = 5\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer CV\n",
    "outer_cv = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results of each outer fold\n",
    "outer_fold_results = []\n",
    "mean_fprs = np.linspace(0, 1, 100)  # Define evenly spaced FPR values for interpolation\n",
    "tpr_interpolations = []\n",
    "\n",
    "# Outer Cross-Validation loop\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner CV (hyperparameter tuning)\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object for hyperparameter tuning within each outer fold\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1000,\n",
    "        scoring='roc_auc',\n",
    "        cv=inner_cv,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning on the training data of the outer fold\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    outer_fold_results.append(auroc)\n",
    "\n",
    "    # ROC curve for the outer fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    tpr_interp = np.interp(mean_fprs, fpr, tpr)\n",
    "    tpr_interp[0] = 0.0  # Ensure starting at 0\n",
    "    tpr_interpolations.append(tpr_interp)\n",
    "\n",
    "# Calculate and print the average performance across all outer folds\n",
    "average_performance = np.mean(outer_fold_results)\n",
    "std_dev_performance = np.std(outer_fold_results)\n",
    "print(\"Average AUROC Across Outer Folds:\", average_performance)\n",
    "print(\"Standard Deviation of AUROC Across Outer Folds:\", std_dev_performance)\n",
    "\n",
    "# Compute mean and standard deviation of the interpolated TPR\n",
    "mean_tpr = np.mean(tpr_interpolations, axis=0)\n",
    "mean_tpr[-1] = 1.0  # Ensure ending at 1\n",
    "std_tpr = np.std(tpr_interpolations, axis=0)\n",
    "\n",
    "# Plotting the ROC curves with shading for standard deviation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_fprs, mean_tpr, color='b', label=f'Mean ROC (AUC = {average_performance:.2f})', lw=2)\n",
    "plt.fill_between(mean_fprs, mean_tpr-std_tpr, mean_tpr+std_tpr, color='grey', alpha=0.2, label='±1 std. dev.')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get feature importance\n",
    "\n",
    "feature_importance = best_xgb_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Set custom tick positions\n",
    "tick_positions = np.arange(len(feature_importance)) * 1.5  # Adjust the multiplier to increase spacing\n",
    "\n",
    "plt.barh(tick_positions, feature_importance, tick_label=feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('XGBoost Classifier Feature Importance')\n",
    "plt.yticks(tick_positions, feature_names)  # Apply custom tick positions\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## xgboost PR curves\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# setup\n",
    "X = df_4[inputs_4]\n",
    "y = df_4['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outcome stratification\n",
    "n_splits = 5  # Number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# make RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=1000,  # chose the number of random parameter combinations to try\n",
    "    scoring='average_precision',  # making avg precision our scoring metric\n",
    "    cv=stratified_kfold.split(X, y),  # Use stratified folds for outcome stratification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object on our data for hyperparameter tuning\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# save best hyperparameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# make plot for figure outside of loop to be able to overlay all plots at the end\n",
    "#plt.figure(figsize=(8, 6))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# do stratification by outcome\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # fit model\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    #  probabilities\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # do PR curve vals\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # plot the PR curve for the current cross-validation fold\n",
    "    ax.plot(recalls_list[fold], precisions_list[fold], lw=1, label=f'Fold {fold+1} (AUCPR = {round(auc_pr, 3)})')\n",
    "\n",
    "    # Add AUC-PR to the scores list\n",
    "    scores.append(auc_pr)\n",
    "\n",
    "# calculate average AUC-PR score across all stratified folds\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Print AUROC scores for each stratified fold\n",
    "for i, auprc in enumerate(scores):\n",
    "    print(f\"Stratified Fold {i+1} AUPRC: {auprc:.4f}\")\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "\n",
    "\n",
    "\n",
    "# calculate the mean PR curve - must do sorting and interpolation to fit data correctly\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# plot  mean PR curve\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# plot the chance PR curve (should be a diagonal line from 0,1 to 1,0)\n",
    "#plt.plot([0, 1], [1, 0], color='gray', linestyle='--', lw=2, label='Chance')\n",
    "\n",
    "# Plot baseline prevalence as a horizontal line\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Other plot settings\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('AUPRC Curves for Stratified Cross-Validation Folds (Experiment 4)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost pr curves with group strat + nested cv loops\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup\n",
    "X = df_4[inputs_4]\n",
    "y = df_4['outcome_rvef_low']\n",
    "groups = df_4['mrn']  # Group identifier column\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# Number of splits for outer and inner cross-validation\n",
    "n_splits_outer = 5\n",
    "n_splits_inner = 3\n",
    "\n",
    "# GroupKFold for outer cross-validation\n",
    "outer_group_kfold = GroupKFold(n_splits=n_splits_outer)\n",
    "\n",
    "# Lists to store results\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "average_aucpr_scores = []\n",
    "\n",
    "# Outer cross-validation loop\n",
    "for train_index, test_index in outer_group_kfold.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    groups_train = groups.iloc[train_index]\n",
    "\n",
    "    # XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # GroupKFold for inner cross-validation (hyperparameter tuning)\n",
    "    inner_group_kfold = GroupKFold(n_splits=n_splits_inner)\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,\n",
    "        scoring='average_precision',\n",
    "        cv=inner_group_kfold.split(X_train, y_train, groups_train),\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model for this outer fold\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test set of the outer fold\n",
    "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # AUC-PR\n",
    "    auc_pr = auc(recall, precision)\n",
    "    average_aucpr_scores.append(auc_pr)\n",
    "\n",
    "# Calculate and print the average and standard deviation performance across all outer folds\n",
    "average_aucpr = np.mean(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", average_aucpr)\n",
    "std_aucpr = np.std(average_aucpr_scores)\n",
    "print(\"Average AUPRC Across Group-Stratified Folds:\", std_aucpr)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "precision_interp = []\n",
    "\n",
    "# Interpolate and plot each fold's precision-recall curve\n",
    "for recall, precision in zip(recalls_list, precisions_list):\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recall, precision)))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curves for Group-Stratified Cross-Validation Folds (Experiment 4)')\n",
    "\n",
    "plt.savefig('pr_curve_exp_4.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC for ASE Guidelines Model (AUC = {round(average_aucpr, 3)})')\n",
    "ax.plot(mean_recall4, mean_precision4, color='g', linestyle='-', lw=2, label=f'Mean AUPRC for Expanded Dataset Model (AUC = {round(average_aucpr4, 3)})')\n",
    "\n",
    "\n",
    "\n",
    "baseline_prevalence = y.mean()\n",
    "ax.hlines(baseline_prevalence, 0, 1, colors='red', linestyles='dashed', lw=2, label=f'Baseline Prevalence ({round(baseline_prevalence, 3)})')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Prediction of RVEF < 47%: Precision-Recall Curves')\n",
    "\n",
    "plt.savefig('pr_curves_rvef_overlayed.svg')\n",
    "plt.savefig('pr_curves_rvef_overlayed.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recall4 = mean_recall\n",
    "mean_precision4 = mean_precision\n",
    "average_aucpr4 = average_aucpr\n",
    "baseline_prevalence4 = baseline_prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr curves using nested cv\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# setup\n",
    "X = df_4[inputs_4]\n",
    "y = df_4['outcome_rvef_low']\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [10, 25, 50, 100, 200],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0, 5.0, 10.0, 20.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 5.0, 10, 20],\n",
    "    'min_child_weight': [0.1, 0.5, 1, 5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# make StratifiedKFold cross-validation iterator for outer cross-validation\n",
    "n_outer_splits = 5  # Number of outer splits\n",
    "outer_stratified_kfold = StratifiedKFold(n_splits=n_outer_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Add empty lists to store precision and recall for each fold\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "# Outer loop: iterate over outer folds\n",
    "for outer_fold, (train_index, test_index) in enumerate(outer_stratified_kfold.split(X, y)):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # make XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # make StratifiedKFold cross-validation iterator for inner cross-validation\n",
    "    n_inner_splits = 3  # Number of inner splits\n",
    "    inner_stratified_kfold = StratifiedKFold(n_splits=n_inner_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # make RandomizedSearchCV object\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=500,  # chose the number of random parameter combinations to try\n",
    "        scoring='average_precision',  # making avg precision our scoring metric\n",
    "        cv=inner_stratified_kfold.split(X_train_outer, y_train_outer),  # Use stratified folds for outcome stratification\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # fit RandomizedSearchCV object on training data for hyperparameter tuning\n",
    "    random_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # save best hyperparameters and best model from the random search\n",
    "    best_params = random_search.best_params_\n",
    "    best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "    # fit best model on the entire training set from the outer fold\n",
    "    best_xgb_model.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "    # predict probabilities on the test set from the outer fold\n",
    "    y_pred_prob = best_xgb_model.predict_proba(X_test_outer)[:, 1]\n",
    "\n",
    "    # compute PR curve values\n",
    "    precision, recall, _ = precision_recall_curve(y_test_outer, y_pred_prob)\n",
    "\n",
    "    # store precision and recall per outer fold\n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "\n",
    "    # calculate Area Under the Precision-Recall Curve (AUC-PR)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    # print AUC-PR for the outer fold\n",
    "    print(f\"Outer Fold {outer_fold+1} AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "# calculate the mean PR curve\n",
    "mean_recall = np.linspace(0, 1, 100)  # choose number of thresholds, here chose 100\n",
    "precision_interp = []\n",
    "\n",
    "for fold in range(len(recalls_list)):\n",
    "    # must sort recalls first before interpolation\n",
    "    recall_sorted, precision_sorted = zip(*sorted(zip(recalls_list[fold], precisions_list[fold])))\n",
    "    precision_interp.append(np.interp(mean_recall, recall_sorted, precision_sorted))\n",
    "\n",
    "mean_precision = np.mean(precision_interp, axis=0)\n",
    "\n",
    "# calculate average AUC-PR score across all outer folds\n",
    "scores = [auc(recalls_list[i], precisions_list[i]) for i in range(len(recalls_list))]\n",
    "average_aucpr = np.mean(scores)\n",
    "print(\"Average AUPRC Across Stratified Outer Folds:\", average_aucpr)\n",
    "\n",
    "std_aucpr = np.std(scores)\n",
    "print(\"Standard Deviation AUPRC Across Stratified Outer Folds:\", std_aucpr)\n",
    "\n",
    "# plot mean PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_recall, mean_precision, color='b', linestyle='-', lw=2, label=f'Mean AUPRC (AUC = {round(average_aucpr, 3)})')\n",
    "\n",
    "# Other plot settings\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('AUPRC Curve for Nested Cross-Validation with RandomizedSearchCV')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
